# Configuration for small-scale experiments

model:
  name: "clip"
  pretrained: "openai/clip-vit-base-patch32"
  temporal_modeling: true
  max_frames: 16
  frame_sampling: "uniform"
  freeze_clip: false

data:
  video_dir: "data/videos"
  annotation_file: "data/annotations.json"
  batch_size: 4
  num_workers: 2
  video_fps: 2.0
  image_size: 224

training:
  epochs: 50
  learning_rate: 2e-4
  weight_decay: 1e-5
  warmup_steps: 500
  gradient_accumulation_steps: 1
  mixed_precision: true
  save_every: 5
  eval_every: 2

evaluation:
  metrics: ["r@1", "r@5", "r@10", "mAP", "temporal_consistency"]
  iou_threshold: 0.5
  temporal_window: 3

device:
  device: "auto"
  mixed_precision_dtype: "auto"

logging:
  log_dir: "logs"
  tensorboard: true
  wandb: false
  project_name: "video-moment-retrieval-small"

paths:
  checkpoint_dir: "checkpoints"
  output_dir: "outputs"
  asset_dir: "assets"
